Rank 6 (local 6) binding to cpus: [33, 34, 35, 36, 37, 38, 39]
Using PyTorch version: 2.3.1+rocm6.0
Rank 6 of 8 (local: 6) sees 8 devices
Using GPU, device name: AMD Instinct MI250X
Rank 5 (local 5) binding to cpus: [9, 10, 11, 12, 13, 14, 15]
Using PyTorch version: 2.3.1+rocm6.0
Rank 5 of 8 (local: 5) sees 8 devices
Using GPU, device name: AMD Instinct MI250X
Rank 2 (local 2) binding to cpus: [17, 18, 19, 20, 21, 22, 23]
Using PyTorch version: 2.3.1+rocm6.0
Rank 2 of 8 (local: 2) sees 8 devices
Using GPU, device name: AMD Instinct MI250X
Rank 7 (local 7) binding to cpus: [41, 42, 43, 44, 45, 46, 47]
Using PyTorch version: 2.3.1+rocm6.0
Rank 7 of 8 (local: 7) sees 8 devices
Using GPU, device name: AMD Instinct MI250X
Rank 1 (local 1) binding to cpus: [57, 58, 59, 60, 61, 62, 63]
Using PyTorch version: 2.3.1+rocm6.0
Rank 1 of 8 (local: 1) sees 8 devices
Using GPU, device name: AMD Instinct MI250X
Rank 3 (local 3) binding to cpus: [25, 26, 27, 28, 29, 30, 31]
Using PyTorch version: 2.3.1+rocm6.0
Rank 3 of 8 (local: 3) sees 8 devices
Using GPU, device name: AMD Instinct MI250X
Rank 4 (local 4) binding to cpus: [1, 2, 3, 4, 5, 6, 7]
Using PyTorch version: 2.3.1+rocm6.0
Rank 4 of 8 (local: 4) sees 8 devices
Using GPU, device name: AMD Instinct MI250X
Rank 0 (local 0) binding to cpus: [49, 50, 51, 52, 53, 54, 55]
Using PyTorch version: 2.3.1+rocm6.0
Rank 0 of 8 (local: 0) sees 8 devices
Using GPU, device name: AMD Instinct MI250X
Loading model and tokenizer
Loading model and tokenizer took: 42.18 seconds
Length of dataset (tokenized) 121478
{'eval_loss': 3.2146406173706055, 'eval_runtime': 4.343, 'eval_samples_per_second': 230.256, 'eval_steps_per_second': 0.23, 'epoch': 0.05}
{'eval_loss': 3.199169874191284, 'eval_runtime': 3.9701, 'eval_samples_per_second': 251.883, 'eval_steps_per_second': 0.252, 'epoch': 0.11}
{'loss': 3.2026, 'grad_norm': 1.5162060260772705, 'learning_rate': 1e-05, 'epoch': 0.13}
{'eval_loss': 3.1877987384796143, 'eval_runtime': 4.0305, 'eval_samples_per_second': 248.111, 'eval_steps_per_second': 0.248, 'epoch': 0.16}
{'eval_loss': 3.1785895824432373, 'eval_runtime': 3.9701, 'eval_samples_per_second': 251.882, 'eval_steps_per_second': 0.252, 'epoch': 0.21}
{'loss': 3.1718, 'grad_norm': 1.4284709692001343, 'learning_rate': 0.0, 'epoch': 0.26}
{'eval_loss': 3.1740479469299316, 'eval_runtime': 10.8616, 'eval_samples_per_second': 92.067, 'eval_steps_per_second': 0.092, 'epoch': 0.26}
{'train_runtime': 863.8157, 'train_samples_per_second': 37.045, 'train_steps_per_second': 1.158, 'train_loss': 3.1872020263671876, 'epoch': 0.26}

Training done, you can find all the model checkpoints in /scratch/project_462000365/odop_ml_benchmark_data/gpt-imdb-model-multigpu
Perplexity on validation: 23.90
Perplexity on test: 23.78
